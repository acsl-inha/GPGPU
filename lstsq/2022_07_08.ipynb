{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022_07_08.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMISwGVPRbkfj1iDH1wdvEg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rdFijFDKYi35"},"outputs":[],"source":["!pip install pycuda"]},{"cell_type":"code","source":["import pycuda.autoinit\n","import pycuda.driver as drv\n","from pycuda import gpuarray\n","from pycuda.compiler import SourceModule\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from time import time\n","import math"],"metadata":{"id":"2lKdf8onY0aY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Shared:\n","    def __init__(self, A, b, learning_rate):\n","        ## input matrix's shape\n","        self.length = A.shape[0]\n","        self.width = A.shape[1]\n","        \n","        ## in CPU\n","        self.A = np.float32(A)\n","        self.b = np.float32(b)\n","        self.theta = np.float32(np.zeros(self.width))\n","        self.out = np.float32(np.zeros(self.length))\n","        self.grad = np.float32(np.zeros(self.width))\n","        self.constrained = constrained\n","\n","        ## in GPU\n","        self.GPU_A = gpuarray.to_gpu(self.A.reshape(self.length*self.width))\n","        self.GPU_b = gpuarray.to_gpu(self.b)\n","        self.GPU_theta = gpuarray.to_gpu(self.theta)\n","        self.GPU_out = gpuarray.to_gpu(self.out)\n","        self.GPU_grad = gpuarray.to_gpu(self.grad)\n","\n","        ## for initialize out vector and grad vector\n","        self.init_out = gpuarray.empty_like(self.GPU_out)\n","        self.init_grad = gpuarray.empty_like(self.GPU_grad)\n","\n","        ## TPB: thread_per_block, BPG: block_per_grid        \n","        self.TPB, self.BPG = self.optimal_block_size(self.length)\n","\n","        ## learning_rate\n","        self.learning_rate = learning_rate\n","\n","    def optimal_block_size(self, n):\n","        \n","        thread_per_block = int(math.sqrt(n / 2))\n","\n","        block_per_grid = int(n / thread_per_block) + 1\n","\n","\n","        return thread_per_block, block_per_grid\n","\n","    def momentum(self):\n","        self.s = self.learning_rate\n","        self.beta = 1/3\n","\n","    def nesterov(self):\n","        return NotImplementedError()"],"metadata":{"id":"mGeKW818Y4lP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GetGradient:\n","    def __init__(self, shared):\n","       self.shared = shared\n","       \n","       self.kernel_function()\n","\n","    def run(self):\n","\n","        self.initialize()\n","\n","        ## get out = np.dot(A, theta) - b\n","        self.first(self.shared.GPU_out,\n","                   self.shared.GPU_A,\n","                   self.shared.GPU_theta,\n","                   self.shared.GPU_b,\n","                   np.int32(self.shared.length),\n","                   np.int32(self.shared.width),\n","                   block=(self.shared.TPB,1,1),\n","                   grid=(self.shared.BPG,1,1))\n","\n","        ## get grad = np.dot(A.T, out)\n","        self.second(self.shared.GPU_grad,\n","                   self.shared.GPU_A,\n","                   self.shared.GPU_out,\n","                   np.int32(self.shared.TPB),\n","                   np.int32(self.shared.BPG),\n","                   np.int32(self.shared.length),\n","                   np.int32(self.shared.width),\n","                   block=(self.shared.BPG,1,1),\n","                   grid=(self.shared.width,1,1))\n","                   \n","                   \n","\n","    def kernel_function(self):\n","        ## block=(thread_per_block,1,1), grid=(block_per_grid,1,1)\n","        first_ker_function = \\\n","        \"\"\"\n","        #define x (threadIdx.x + blockIdx.x * blockDim.x)\n","\n","        __global__ void first(float* out, float* A, float* theta, float* b, int length, int width) {\n","            \n","            if (x < length) {\n","                for (int j = 0; j < width; j++) {\n","                    int index1 = x * width + j;\n","\n","                    out[x] += A[index1] * theta[j];\n","                    }\n","\n","                out[x] -= b[x];\n","            }\n","        }\n","        \"\"\"\n","        first_ker = SourceModule(first_ker_function)\n","\n","\n","\n","        ## block=(block_per_grid,1,1), grid=(width,1,1)\n","        second_ker_function = \\\n","        \"\"\"\n","        #define tx (threadIdx.x)\n","        #define bx (blockIdx.x)\n","\n","        __global__ void second(float* grad, float* A, float* out, int thread_per_block, int block_per_grid, int length, int width) {\n","\n","            __shared__ float grad_jerk[1000];\n","\n","            grad_jerk[tx] = 0;\n","\n","            __syncthreads();\n","            \n","            for (int i = 0; i < thread_per_block; i++) {\n","                int index1 = tx * thread_per_block + i;\n","                int index2 = index1 * width + bx;\n","                \n","                grad_jerk[tx] += A[index2] * out[index1];\n","            }\n","\n","            __syncthreads();\n","\n","            if (tx == 0) {\n","                for (int i = 0; i < block_per_grid; i++) {\n","                    grad[bx] += grad_jerk[i];\n","                }\n","            }\n","            else {\n","                grad_jerk[1000-tx] = 0;\n","            }\n","\n","            __syncthreads();\n","        }\n","        \"\"\"\n","        second_ker = SourceModule(second_ker_function)\n","\n","        self.first = first_ker.get_function(\"first\")\n","        self.second = second_ker.get_function(\"second\")\n","\n","    def initialize(self):\n","        self.shared.GPU_out[:] = self.shared.init_out[:]"],"metadata":{"id":"QHA3A-UnY_bP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Optimizer:\n","    def __init__(self, shared):\n","        self.shared = shared\n","\n","        self.kernel_function()\n","\n","    def run(self):\n","        return NotImplementedError()\n","\n","    def kernel_function(self):\n","        return NotImplementedError()\n","        \n","    def initialize(self):\n","        return NotImplementedError()\n","\n","\n","\n","class GradientMethod(Optimizer):\n","    def __init__(self, shared):\n","        super().__init__(shared)\n","\n","    def run(self):\n","\n","        self.gradient_method(self.shared.GPU_theta,\n","                             self.shared.GPU_grad,\n","                             np.float32(self.shared.learning_rate),\n","                             block=(self.shared.width,1,1),\n","                             grid=(1,1,1))\n","\n","        self.initialize()\n","        \n","    def kernel_function(self):\n","\n","        ## block=(width,1,1), grid=(1,1,1)\n","        gradient_method_ker = SourceModule(\n","            \"\"\"\n","            #define x (threadIdx.x)\n","\n","            __global__ void gradient_method (float* theta, float* grad, float learning_rate) {\n","                theta[x] -= learning_rate * grad[x];\n","\n","                __syncthreads();\n","            }\n","            \"\"\"\n","        )\n","\n","        self.gradient_method = gradient_method_ker.get_function(\"gradient_method\")\n","\n","    def initialize(self):\n","        self.shared.GPU_grad[:] = self.shared.init_grad[:]\n","\n","\n","\n","class MomentumMethod(Optimizer):\n","    def __init__(self, shared):\n","        super().__init__(shared)\n","\n","    def run(self):\n","\n","        self.momentum_method(self.shared.GPU_theta,\n","                             self.shared.GPU_grad,\n","                             np.float32(self.shared.s),\n","                             block=(self.shared.width,1,1),\n","                             grid=(1,1,1))\n","                             \n","        self.initialize()\n","\n","    def kernel_function(self):\n","\n","        ## block=(width,1,1), grid=(1,1,1)\n","        momentum_method_ker = SourceModule(\n","            \"\"\"\n","            #define x (threadIdx.x)\n","\n","            __global__ void momentum_method (float* theta, float* grad, float s) {\n","                theta[x] -= s * grad[x];\n","\n","                __syncthreads();\n","            }\n","            \"\"\"\n","        )\n","        \n","        ## block=(width,1,1), grid=(1,1,1)\n","        momentum_ker = SourceModule(\n","            \"\"\"\n","            #define x (threadIdx.x)\n","\n","            __global__ void momentum (float* grad, float beta) {\n","\n","                grad[x] *= beta;\n","            }\n","            \"\"\"\n","        )\n","\n","        self.momentum_method = momentum_method_ker.get_function(\"momentum_method\")\n","        self.momentum = momentum_ker.get_function(\"momentum\")\n","\n","    def initialize(self):\n","        self.momentum(self.shared.GPU_grad,\n","                      np.float32(self.shared.beta),\n","                      block=(self.shared.width,1,1),\n","                      grid=(1,1,1))"],"metadata":{"id":"cAQcWShDZFtP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A = np.random.rand(2000,50)\n","b = np.random.rand(2000)\n","learning_rate = 3e-5"],"metadata":{"id":"uxoP16p7ZJFF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t1 = time()\n","lstsq = LeastSquare(A, b, learning_rate, iteration=5, optimize_method=\"momentum\")\n","lstsq.solve()\n","opt_theta_gpu = lstsq.shared.GPU_theta.get()\n","t2 = time()\n","\n","print(f\"Duration: {round(t2 - t1,  4)} s\")\n","\n","plt.plot(lstsq.error)\n","plt.show()\n","\n","print(lstsq.error[-1])"],"metadata":{"id":"-TtydrlMZKaS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t1 = time()\n","opt_theta = np.linalg.lstsq(A,b, rcond=None)[0]\n","t2 =time()\n","\n","print(f\"Duration: {round(t2 - t1, 4)} s\")\n","\n","print(np.linalg.norm(np.dot(A, opt_theta) - b))"],"metadata":{"id":"35kvmo4_ZMQx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["error = np.zeros((10,2))\n","duration = np.zeros((10,2))\n","\n","for i in range(10):\n","    length = (i+1) * 1000\n","    A = np.random.randn(length,50)\n","    b = np.random.randn(length)\n","    learning_rate = 2e-5 * i\n","\n","    t1 = time()\n","    lstsq = LeastSquare(A, b, learning_rate, iteration=5, optimize_method=\"momentum\")\n","    lstsq.solve()\n","    opt_theta_gpu = lstsq.shared.GPU_theta.get()\n","    t2 = time()\n","\n","    error[i,0] = np.linalg.norm(np.dot(A, opt_theta_gpu) - b)\n","    duration[i,0] = t2 - t1\n","\n","    t1 = time()\n","    opt_theta = np.linalg.lstsq(A,b, rcond=None)[0]\n","    t2 =time()\n","\n","    error[i,1] = np.linalg.norm(np.dot(A, opt_theta) - b)\n","    duration[i,1] = t2 - t1\n","\n","plt.figure(figsize=(16,8))\n","\n","plt.subplot(121)\n","plt.plot(error[:,0], label=\"error by GPU\")\n","plt.plot(error[:,1], label=\"error by CPU\")\n","plt.legend()\n","\n","plt.subplot(122)\n","plt.plot(duration[:,0], label=\"duration in GPU\")\n","plt.plot(duration[:,1], label=\"duration in CPU\")\n","plt.legend()\n","\n","plt.show()"],"metadata":{"id":"GwDl_2ElZW2V"},"execution_count":null,"outputs":[]}]}