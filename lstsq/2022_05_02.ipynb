{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022_05_02.ipynb","provenance":[],"collapsed_sections":["kao0w0arfHuW","qGUhqpZ9gRHu"],"authorship_tag":"ABX9TyObnTb/R0+Dvc0QU9asq+Zy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# SGD\n","\n","[GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training](\"https://arxiv.org/abs/1312.6186\") 참고...\n"],"metadata":{"id":"kao0w0arfHuW"}},{"cell_type":"markdown","source":["## 2. Training neural networks"],"metadata":{"id":"qGUhqpZ9gRHu"}},{"cell_type":"markdown","source":["In traditional gradient descent, __the gradeint of the objective function__ needs to be calculated over the entire dataset. The parameters are then __updated__ with this gradient. This is repeated until convergence. There are two __main issues__ with this approach.<br>\n","1. The dataset may be too large to fit into memory.\n","2. The gradient may take too long to compute.\n","\n","When the dataset is too large, __stochastic gradient descent (SGD)__ may be used.<br>\n","SGD can be accelerated in two ways:<br>\n","1. speeding up the calculation of the minibatch gradient (__model parallelism__)\n","2. parallelization of the stochastic gradient descent steps (__data parallelism__)\n"],"metadata":{"id":"YcAICV8ihHoz"}}]}