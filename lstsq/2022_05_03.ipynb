{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022_05_03.ipynb","provenance":[],"collapsed_sections":["GTKPxD7cEZGT","zXjoXMWhC4eI","uBTs-tCA_ubK","JyJHr9qyEJY8","08p3JyYtJukF"],"authorship_tag":"ABX9TyP/WxvpmhTEW90etLQZ1+Uu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/Colab Notebooks/lstsq/Pycuda')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mo11hCtJXfAV","executionInfo":{"status":"ok","timestamp":1651668522594,"user_tz":-540,"elapsed":25238,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"e4c140a8-ffc0-4938-a51b-64232e1fc36e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["밑에 사용한 방식은 GPU를 두개를 쓴다는 가정하에 각 GPU가 전체 데이터 셋을 나누어 가져간 후, 서로 서로 SGD를 통해 학습하고 매 epoch 마다 결과를 합쳐(평균을 계산) 다시 그 결과를 통해 학습하는 방법입니다. 앞서 읽은 논문에서의 방법으로 표현하면 __data parallelism__에 해당하는 방법입니다.."],"metadata":{"id":"BuqmO5U4XzQ2"}},{"cell_type":"markdown","source":["```python\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from time import time \n","\n","class LeastSquare():\n","    def __init__(self, A, b, num_gpu=2, epoches=2):\n","        self.A = A\n","        self.b = b\n","        self.lr = 1e-3/A.shape[1]\n","        self.num_gpu = num_gpu\n","        self.epoches = epoches\n","        ## record each gpu's optimized x\n","        self.x_hat = np.random.rand(A.shape[1])\n","        self.x_list = np.zeros((self.num_gpu,self.A.shape[1]))\n","        self.error_list = []\n","        self.n = int(self.A.shape[0] / num_gpu)\n","        self.A1 = A[:self.n,:]\n","        self.b1 = b[:self.n]\n","        self.A2 = A[self.n:,:]\n","        self.b2 = b[self.n:]\n","\n","    def run(self):        \n","        for i in range(self.epoches):\n","            x = self.x_hat\n","\n","            for j in range(self.num_gpu):\n","                A, b = self.initialize(j)\n","                x_ = self.optimize(A, b, x)\n","                self.x_list[j,:] = x_\n","                error = self.check(x_)\n","\n","            self.x_hat = np.sum(self.x_list, axis=0) / self.num_gpu\n","\n","        return self.x_hat\n","\n","    ## initialize\n","    def initialize(self, num_gpu):\n","        index = np.random.choice(self.n,1000)\n","        if num_gpu == 0:\n","            A = self.A1[index,:]\n","            b = self.b1[index]\n","        else:\n","            A = self.A2[index,:]\n","            b = self.b2[index]\n","\n","        return A, b\n","\n","    def optimize(self, A, b, x, iters_per_epoch=500):\n","        ## optimize x\n","        for k in range(iters_per_epoch):\n","            b_ = np.dot(A, x)\n","            grad = 2 * np.dot(A.T, (b_ - b))\n","            x -= grad * self.lr\n","\n","        return x\n","\n","    def check(self, x):\n","        b_ = self.A @ x\n","        error = np.linalg.norm(self.b - b_)\n","        self.error_list.append(error)\n","\n","        return error\n","\n","if __name__ == \"__main__\":\n","    A = np.random.rand(10000,1000)\n","    b = np.random.rand(10000)\n","    epoch = 40\n","\n","    t1 = time()\n","    lstsq = LeastSquare(A,b,epoches=epoch)\n","    t2 = time()\n","    dump_time1 = t2 - t1\n","\n","    t1 = time()\n","    theta = lstsq.run()\n","    error = lstsq.check(theta)\n","    t2 = time()\n","    calculation_time = t2 - t1\n","    \n","    t1 = time()\n","    x = np.linalg.lstsq(A, b ,rcond=None)[0]\n","    lstsq_error = np.linalg.norm(lstsq.A @ x - lstsq.b)\n","    t2 = time()\n","    lstsq_time = t2 - t1\n","\n","    t1 = time()\n","    result = open(\"data_parallel_result.txt\", \"w\")\n","    result.write(f\"error: {error}\")\n","    result.write(\"\\n\")\n","    result.write(f\"lstsq error: {lstsq_error}\")\n","    result.write(\"\\n\")\n","    result.write(f\"GPU1 error: {lstsq.error_list[-2]}\")\n","    result.write(\"\\n\")\n","    result.write(f\"GPU2 error: {lstsq.error_list[-1]}\")\n","    result.write(\"\\n\")\n","    result.write(f\"optimal x: {theta}\")\n","    result.close()\n","    t2 = time()\n","    dump_time2 = t2 - t1\n","\n","    t1 = time()\n","    fig = plt.figure(figsize=(16,8))\n","    plt.subplot(121)\n","    plt.plot(lstsq.error_list[::2])\n","    plt.xlabel(\"epoches\")\n","    plt.ylabel(\"error\")\n","    plt.subplot(122)\n","    plt.plot(lstsq.error_list[1::2])\n","    plt.xlabel(\"epoches\")\n","    plt.ylabel(\"error\")\n","    plt.savefig(\"data_parallel_error.png\", dpi=fig.dpi)\n","    t2 = time()\n","    dump_time3 = t2 - t1\n","\n","    dump_time = dump_time1 + dump_time2 + dump_time3\n","\n","    print(f\"It took {calculation_time} seconds to calculate the least square probelm.\")\n","    print(f\"It took {dump_time} seconds to something else.\")\n","    print(f\"It took {lstsq_time} seconds to calculate the np.linalg.lstsq.\")\n","    print(f\"rms between x and theta: {np.linalg.norm(x - theta)}\")\n","```"],"metadata":{"id":"PYImFDQuWrMP"}},{"cell_type":"markdown","source":["data_parallel_result는 계산 결과를 담은 txt파일입니다."],"metadata":{"id":"Kd0YIBpPZa9v"}},{"cell_type":"code","source":["lstsq_result = open(\"data_parallel_result.txt\", \"r\", encoding=\"utf-8\")\n","for i in range(10):\n","    line = lstsq_result.readline()\n","    print(line)\n","lstsq_result.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBiXNVNQX1CZ","executionInfo":{"status":"ok","timestamp":1651639468997,"user_tz":-540,"elapsed":351,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"e8b363d6-ecc7-48bb-897f-87db5c75abb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["error: 28.8149046710103\n","\n","lstsq error: 27.347241765250775\n","\n","GPU1 error: 28.753315811131504\n","\n","GPU2 error: 28.8149046710103\n","\n","optimal x: [ 1.01048599e-02 -1.63962344e-02  8.18530410e-03 -3.00432717e-04\n","\n"," -1.13814665e-02  4.82772075e-03  2.31549080e-02  1.78982011e-02\n","\n","  1.54345318e-02  2.03025957e-02  2.04379305e-02 -2.31554456e-02\n","\n"," -1.19348363e-02  1.61581271e-02  4.95154600e-04 -3.03585364e-03\n","\n"," -2.88226547e-03  5.94086178e-03  2.13699856e-02 -1.58383437e-02\n","\n","  2.85404116e-02  1.40183391e-02 -3.37064614e-02  4.07298851e-02\n","\n"]}]},{"cell_type":"markdown","source":["data_parallel_profile은 CPU에서 각 계산 과정의 소요시간은 세부적으로 기록한 txt 파일입니다."],"metadata":{"id":"8ydSSVHFZhOf"}},{"cell_type":"code","source":["lstsq_result = open(\"data_parallel_profile.txt\", \"r\", encoding=\"utf-16\")\n","for i in range(10):\n","    line = lstsq_result.readline()\n","    print(line)\n","lstsq_result.close()\n","\n","## cpu 25% 정도 사용하였을때...."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a2mKz6d8Y8rY","executionInfo":{"status":"ok","timestamp":1651639601901,"user_tz":-540,"elapsed":9,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"6fcfbf2d-0da4-4c38-a911-aba01f3b9741"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["It took 20.454949855804443 seconds to calculate the least square probelm.\n","\n","It took 0.6010358333587646 seconds to something else.\n","\n","It took 1.4236323833465576 seconds to calculate the np.linalg.lstsq.\n","\n","rms between x and theta: 0.38661562483931255\n","\n","         1045900 function calls (1033597 primitive calls) in 23.393 seconds\n","\n","\n","\n","   Ordered by: cumulative time\n","\n","\n","\n","   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n","\n","    612/1    0.002    0.000   23.394   23.394 {built-in method builtins.exec}\n","\n"]}]},{"cell_type":"markdown","source":["이는 데이터 셋의 크기를 100배((십만,만)으로) 증가시켜 실행시킨 결과입니다. 이때는 epoch을 40이 아닌 20으로 주어 앞선 결과보다는 덜 수렴한 모습입니다."],"metadata":{"id":"Xu60ce4wZsJ7"}},{"cell_type":"code","source":["lstsq_result = open(\"data_parallel_result_2.txt\", \"r\", encoding=\"utf-8\")\n","for i in range(10):\n","    line = lstsq_result.readline()\n","    print(line)\n","lstsq_result.close()\n","\n","## It took 10.855637550354004 seconds to calculate the least square probelm. \n","## It took 0.6464803218841553 seconds to something else.                                                                   \n","## It took 9.001726627349854 seconds to calculate the np.linalg.lstsq. \n","## rms between x and theta: 0.9537647484008444"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uSkNNiQNX_6f","executionInfo":{"status":"ok","timestamp":1651639471146,"user_tz":-540,"elapsed":818,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"8099a596-18e1-48ac-8dc3-8c86d8251591"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["error: 124.77371425213892\n","\n","lstsq error: 90.8337995556563\n","\n","GPU1 error: 122.82445530120378\n","\n","GPU2 error: 124.77371425213892\n","\n","optimal x: [-4.42302010e-02  3.79321436e-02 -1.13119200e-02  1.14975630e-02\n","\n","  3.34142908e-02  1.68574959e-02  6.22364496e-02  1.63532743e-02\n","\n","  1.13642179e-02  1.76564975e-02 -2.60937915e-02  1.06707097e-02\n","\n","  3.32064801e-02 -3.52888164e-05 -4.70384507e-02 -2.18729501e-02\n","\n","  2.03667184e-02  1.70091327e-02 -1.94876913e-02 -3.78571168e-03\n","\n","  5.15585398e-02 -7.76529844e-04  1.13756708e-02  1.54584058e-02\n","\n"]}]},{"cell_type":"markdown","source":["이후 내용은 PYCUDA 책을 읽으면서 내용을 정리한 노트입니다."],"metadata":{"id":"jxx-6Ul0Z-mx"}},{"cell_type":"markdown","source":["# Querying your GPU"],"metadata":{"id":"GTKPxD7cEZGT"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"55F_Wwjs5I0O","executionInfo":{"status":"ok","timestamp":1651668001646,"user_tz":-540,"elapsed":109400,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"58ec4b87-0a0f-4bb1-82dc-2138b4831497"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pycuda\n","  Downloading pycuda-2021.1.tar.gz (1.7 MB)\n","\u001b[?25l\r\u001b[K     |▏                               | 10 kB 31.7 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 337 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 348 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 358 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 368 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 378 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 389 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 399 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 409 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 419 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 430 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 440 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 450 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 460 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 471 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 481 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 491 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 501 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 512 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 522 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 532 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 542 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 552 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 563 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 573 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 583 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 593 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 604 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 614 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 624 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 634 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 645 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 655 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 665 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 675 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 686 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 696 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 706 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 716 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 727 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 737 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 747 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 757 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 768 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 778 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 788 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 798 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 808 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 819 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 829 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 839 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 849 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 860 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 870 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 880 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 890 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 901 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 911 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 921 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 931 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 942 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 952 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 962 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 972 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 983 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 993 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.7 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.7 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 5.1 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting mako\n","  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 8.7 MB/s \n","\u001b[?25hRequirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.4.4)\n","Collecting pytools>=2011.2\n","  Downloading pytools-2022.1.6.tar.gz (68 kB)\n","\u001b[K     |████████████████████████████████| 68 kB 9.1 MB/s \n","\u001b[?25hCollecting platformdirs>=2.2.0\n","  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (1.21.6)\n","Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (4.2.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (2.0.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->mako->pycuda) (3.8.0)\n","Building wheels for collected packages: pycuda, pytools\n","  Building wheel for pycuda (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycuda: filename=pycuda-2021.1-cp37-cp37m-linux_x86_64.whl size=626634 sha256=fe84bfe8c91699e4a7b930ecf766d8cda2a9cc9dd6e5c4eae24b3d9d2fb0d190\n","  Stored in directory: /root/.cache/pip/wheels/c4/ef/49/dc6a5feb8d980b37c83d465ecab24949a6aa19458522a9e001\n","  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytools: filename=pytools-2022.1.6-py2.py3-none-any.whl size=64614 sha256=dbf483927a1bc23f5b6ab09c694bb278c599855bd45ce546f6e52889194c245b\n","  Stored in directory: /root/.cache/pip/wheels/11/99/b3/ec23c6be538defad70612abbedac1ac2b78213c156a304c313\n","Successfully built pycuda pytools\n","Installing collected packages: platformdirs, pytools, mako, pycuda\n","Successfully installed mako-1.2.0 platformdirs-2.5.2 pycuda-2021.1 pytools-2022.1.6\n"]}],"source":["!pip install pycuda"]},{"cell_type":"markdown","source":["## Querying your GPU with PyCUDA"],"metadata":{"id":"zXjoXMWhC4eI"}},{"cell_type":"code","source":["import pycuda.driver as drv\n","## always complie this line or import the pycuda.autoinit\n","drv.init()"],"metadata":{"id":"8M0AYhvV530G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Detected {drv.Device.count()} CUDA Capable device (s)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"td0rK1nl58rP","executionInfo":{"status":"ok","timestamp":1651581230751,"user_tz":-540,"elapsed":4,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"701ec4ca-b6df-4707-d3f3-f3b59c3d4432"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected 1 CUDA Capable device (s)\n"]}]},{"cell_type":"markdown","source":["pycuda.driver.Device: instance\n","with methods like:<br>\n","1. __count__: count available devices\n","2. __compute_capability__: print device's compute capability\n","3. __total_memory__: print(total amount of device memory "],"metadata":{"id":"0XOsZ3qC7ZPq"}},{"cell_type":"code","source":["for i in range(drv.Device.count()):\n","    gpu_device = drv.Device(i)\n","    print(f\"Device {i}: {gpu_device.name()}\")\n","    compute_capability = float(\"%d.%d\" % gpu_device.compute_capability())\n","    print(f\"\\t Compute Capability: {compute_capability}\")\n","    print(f\"\\t Total Memory: {gpu_device.total_memory() // (1024 ** 2)} megabytes\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rs2s0nL86S8-","executionInfo":{"status":"ok","timestamp":1651581468838,"user_tz":-540,"elapsed":4,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"9c02eeb1-9032-4da8-9b17-8b68fc53a622"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device 0: Tesla T4\n","\t Compute Capability: 7.5\n","\t Total Memory: 15109 megabytes\n"]}]},{"cell_type":"markdown","source":["with __get_attributes__ method, can look out GPU's attributes and it returns in python dictionary type."],"metadata":{"id":"buuWJMhr9IA-"}},{"cell_type":"code","source":["device_attributes_tuples = gpu_device.get_attributes().items()\n","device_attributes = {}\n","for k, v in device_attributes_tuples:\n","    device_attributes[str(k)] = v"],"metadata":{"id":"OH33pvEz7T0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_mp = device_attributes[\"MULTIPROCESSOR_COUNT\"]"],"metadata":{"id":"Rw1pjdK48f_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cuda_cores_per_mp = {5.0: 128, 5.1: 128, 5.2: 128, 6.0: 64, 6.1: 128, 6.2: 128, 7.5: 128}[compute_capability]"],"metadata":{"id":"FfPlEoTw9E9b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"\\t ({num_mp}) Multiprocessors, ({cuda_cores_per_mp}) CUDA Cores / Multiprocessors: {num_mp * cuda_cores_per_mp} CUDA Cores\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qGqMsSpB-1RV","executionInfo":{"status":"ok","timestamp":1651582626432,"user_tz":-540,"elapsed":3,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"2fa06db9-364f-4af5-9bfc-cd9e13a4deaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\t (40) Multiprocessors, (128) CUDA Cores / Multiprocessors: 5120 CUDA Cores\n"]}]},{"cell_type":"markdown","source":["# Using PyCUDA's gpuarray class"],"metadata":{"id":"uBTs-tCA_ubK"}},{"cell_type":"markdown","source":["Like NumPy's array class, PyCUDA's gpuarray class plays an analogously prominent role within GPU programming in Python.<br>\n","This has all of the features you know and love from NumPy:<br>\n","1. __multidimensional vector/matrix/tensor shape structuring__\n","2. __array-slicing, array unraveling__\n","3. __overloaded operators for point-wise computations__"],"metadata":{"id":"XRzYziJdDAQJ"}},{"cell_type":"markdown","source":["## Transferring data to and from the GPU with gpuarray"],"metadata":{"id":"JyJHr9qyEJY8"}},{"cell_type":"markdown","source":["GPU has its own memory apart from the host computer's memory, which is known as __device memory__(Sometimes this is known more specifically as __global device memory__, to differentiate this from the additional cache memory, shared memory, and register memory that is also on the GPU.)<br>\n","Unlike malloc and free functions in __C__ or new and delete operators in __C++__, in CUDA, this is comlicated further with the additional task of transferring data back and forth between the CPU to the GPU (commands such as __cudaMemcpyHostToDevice__ and __cudaMemcpyDevicetoHost__). \n","> cudaMalloc: memory allocations<br>\n","> cudaFree: deallocations\n","\n","Fortunately, PyCUDA covers all of the overhead of memory allocation, deallocation, and data tranfers with the __gpuarray__ class."],"metadata":{"id":"gAWjRDECEhhK"}},{"cell_type":"code","source":["import numpy as np\n","import pycuda.autoinit\n","from pycuda import gpuarray\n","from time import time"],"metadata":{"id":"zJLz0m_DHTqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["host_data = np.array([1,2,3,4,5], dtype=np.float32)\n","device_data = gpuarray.to_gpu(host_data)\n","device_data_x2 = 2 * device_data\n","host_data_x2 = device_data_x2.get()\n","print(host_data_x2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KiqNqITHH9EP","executionInfo":{"status":"ok","timestamp":1651668011756,"user_tz":-540,"elapsed":1100,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"71c22595-2d74-4437-dfe9-c160c473e14b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 2.  4.  6.  8. 10.]\n"]}]},{"cell_type":"markdown","source":["One thing to note:<br>\n","> set array element's type specifically\n","we set type as np.float32, this corresponds directly with the float type in __C/C++__.\n","\n","It has two denefits.<br>\n","1. We can reduce unnecessary overhead of using an unnecessary type that will possibly take up more computational time or memory.\n","2. We will soon be writing portions of code in inline CUDA C, we will have to be very specific with types or our code won't work correctly, keeping in mind that C is a staticallly-typed language."],"metadata":{"id":"g5IgLtYgINXX"}},{"cell_type":"markdown","source":["## Basic pointwise arithmetic operations with gpuarray"],"metadata":{"id":"08p3JyYtJukF"}},{"cell_type":"markdown","source":["We saw that we can use the Python multiplication operator(*) to multiply each element in a gpuarray object by a scalar value; note...<br>\n","> a pointwise operation is intrinsically parallelizable, and so when we use this operation on a gpuarray object PyCUDA is able to offload each multiplication onto a single thread. The point is that the computation of one element is nor dependent on the computation of any other element."],"metadata":{"id":"6cTv2MnEOg4b"}},{"cell_type":"code","source":["x_host = np.array([1,2,3], dtype=np.float32)\n","y_host = np.array([1,1,1], dtype=np.float32)\n","z_host = np.array([2,2,2], dtype=np.float32)\n","\n","x_device = gpuarray.to_gpu(x_host)\n","y_device = gpuarray.to_gpu(y_host)\n","z_device = gpuarray.to_gpu(z_host)"],"metadata":{"id":"6Mk2jSdnJ42Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_host + y_host"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rACqJy1LKVFr","executionInfo":{"status":"ok","timestamp":1651668020180,"user_tz":-540,"elapsed":429,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"22a21dab-86fb-477b-8c9c-935380483d6d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2., 3., 4.], dtype=float32)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["(x_device + y_device).get()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dgsgedppKWnv","executionInfo":{"status":"ok","timestamp":1651668020867,"user_tz":-540,"elapsed":402,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"df13f7b5-3b5a-458e-ae94-eae6d05ebcd0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2., 3., 4.], dtype=float32)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["x_host ** z_host"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0f7x7w9tKZHA","executionInfo":{"status":"ok","timestamp":1651668020868,"user_tz":-540,"elapsed":5,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"82829d68-e46b-4bcf-d976-c9ea0f717c90"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 4., 9.], dtype=float32)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["(x_device ** z_device).get()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yvZ21X9EKc14","executionInfo":{"status":"ok","timestamp":1651668021189,"user_tz":-540,"elapsed":324,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"34e85675-0ac5-426a-ab40-4111c147f18c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 4., 9.], dtype=float32)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["x_host / x_host"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IUq6p_UTKiPp","executionInfo":{"status":"ok","timestamp":1651668021190,"user_tz":-540,"elapsed":5,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"a9e3694a-2bb6-4192-9a94-d23694f2592e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1.], dtype=float32)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["(x_device / x_device).get()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FMWL6hFDKoo1","executionInfo":{"status":"ok","timestamp":1651668021606,"user_tz":-540,"elapsed":419,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"a2daa304-1fdc-4606-cd0d-a8817f2c90f0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1.], dtype=float32)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["z_host - x_host"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yqaQgEuyKrFF","executionInfo":{"status":"ok","timestamp":1651668021607,"user_tz":-540,"elapsed":8,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"a9bd5740-7445-4180-e669-99544df4b360"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1.,  0., -1.], dtype=float32)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["(z_device - x_device).get()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dzf7NOcOKuA6","executionInfo":{"status":"ok","timestamp":1651668021607,"user_tz":-540,"elapsed":6,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"290983de-2b49-407c-ff5d-e848946d8ca5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1.,  0., -1.], dtype=float32)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["z_host / 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KGf_XJImKw4I","executionInfo":{"status":"ok","timestamp":1651668021607,"user_tz":-540,"elapsed":4,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"7a58a517-e62e-4848-9a8a-74ee4e29c658"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1.], dtype=float32)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["(z_device / 2).get()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bw9RDFvyK0IG","executionInfo":{"status":"ok","timestamp":1651668022269,"user_tz":-540,"elapsed":3,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"12ca4c75-b846-4cb2-d490-b5f022f02ef3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1.], dtype=float32)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["x_host - 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tS_HVmhcK2wT","executionInfo":{"status":"ok","timestamp":1651668022608,"user_tz":-540,"elapsed":1,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"25f8f71b-ef6d-4550-cccb-8c1d1f9f60ee"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 1., 2.], dtype=float32)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["(x_device - 1).get()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jde5Im8kK4pE","executionInfo":{"status":"ok","timestamp":1651668023409,"user_tz":-540,"elapsed":2,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"d3122ae0-e4a7-463b-fe00-8e5ba7d5275f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 1., 2.], dtype=float32)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["host_data = np.float32(np.random.random(50000000))\n","\n","t1 = time()\n","host_data_2x = host_data * np.float32(2)\n","t2 = time()\n","\n","print(f\"total time to compute on CPU: {t2 - t1}\")\n","device_data = gpuarray.to_gpu(host_data)\n","\n","t1 = time()\n","device_data_2x = device_data * np.float32(2)\n","t2 = time()\n","\n","from_device = device_data_2x.get()\n","print(f\"total time to compute on GPU: {t2 - t1}\")\n","\n","print(f\"Is the host computation the same as the GPU computation?: {np.allclose(from_device, host_data_2x)}\")"],"metadata":{"id":"v--pchjuK6p9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651668030646,"user_tz":-540,"elapsed":1379,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"140b043f-4e6c-4213-c7f3-a8bb4cba4f7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total time to compute on CPU: 0.03358745574951172\n","total time to compute on GPU: 0.0008957386016845703\n","Is the host computation the same as the GPU computation?: True\n"]}]},{"cell_type":"code","source":["host_data = np.float32(np.random.random(50000000))\n","\n","t1 = time()\n","host_data_2x = host_data * np.float32(2)\n","t2 = time()\n","\n","print(f\"total time to compute on CPU: {t2 - t1}\")\n","device_data = gpuarray.to_gpu(host_data)\n","\n","t1 = time()\n","device_data_2x = device_data * np.float32(2)\n","t2 = time()\n","\n","from_device = device_data_2x.get()\n","print(f\"total time to compute on GPU: {t2 - t1}\")\n","\n","print(f\"Is the host computation the same as the GPU computation?: {np.allclose(from_device, host_data_2x)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HIyjJeh1Ffcn","executionInfo":{"status":"ok","timestamp":1651668158433,"user_tz":-540,"elapsed":1283,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"075fc489-ee69-4fd3-cb10-95ee840ebade"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total time to compute on CPU: 0.03511476516723633\n","total time to compute on GPU: 0.0007052421569824219\n","Is the host computation the same as the GPU computation?: True\n"]}]},{"cell_type":"code","source":["with open(\"time_calc0.py\", \"r\") as f:\n","    time_calc_code = f.read()"],"metadata":{"id":"dbTVXsXSF__j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%prun -s cumulative exec(time_calc_code)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9NSACNDcHc0k","executionInfo":{"status":"ok","timestamp":1651668585752,"user_tz":-540,"elapsed":765,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"e2dd6643-fbc4-4a18-93ff-765a8059ea7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total time to compute on CPU: 0.03452444076538086\n","total time to compute on GPU: 0.0006413459777832031\n","Is the host computation the same as the GPU computation?: True\n"," "]}]}]}