{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022_07_02.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNopSm+pUQtMiwNcX2gVpi9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Momentum method\n","\n","> $\\nabla f = 2A^T(Ax - b)$<br>\n","> $x_{k+1} = x_k - s \\cdot z_k$<br>\n","> $z_{k+1} = \\nabla f_{k+1} + \\beta \\cdot z_k$\n","\n","$\n","\\begin{bmatrix}\n","1 & 0 \\\\\n","-S & 1\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","x_{k+1} \\\\ z_{k+1}\n","\\end{bmatrix}\n","=\n","\\begin{bmatrix}\n","1 & -s \\\\\n","0 & \\beta\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","x_k \\\\ z_k\n","\\end{bmatrix}\n","$"],"metadata":{"id":"C2qenABtjjMr"}},{"cell_type":"markdown","source":["With assumption that $x_k$ and $z_k$ is tracking S's eigenvector, $s_{optimize}$ and $\\beta_{optimize}$ will be denoted as: <br>\n","$s_{optimize} = ({2 \\over \\sqrt{M} + \\sqrt{m}})^2$<br>\n","$\\beta_{optimize} = ({\\sqrt{M} - \\sqrt{m} \\over \\sqrt{M} + \\sqrt{m}})^2$<br>\n","where $M, m$ is upperbound and downerbound of eigenvalues of S."],"metadata":{"id":"4PRITj64jw42"}},{"cell_type":"markdown","source":["I need to calculate $s_{optimize}$ and $\\beta_{optimize}$ for our object function __$f(x) = \\begin{Vmatrix}Ax - b\\end{Vmatrix}^2$__."],"metadata":{"id":"lfs-0xHCjxEN"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"VdMn5metsByH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## QR decomposition\n","def factorize(A):\n","    width = A.shape[1]\n","\n","    Q = np.zeros_like(A)\n","    R = np.zeros((width,width))\n","\n","    ## first step\n","    norm_q = np.linalg.norm(A[:,0])\n","    Q[:,0] = A[:,0] / norm_q\n","    R[0,0] = norm_q\n","\n","    ## gram-schmidts algorithm\n","    for i in range(1,width):\n","        Q[:,i] = A[:,i]\n","        \n","        for j in range(i):\n","            Q[:,i] -= np.dot(Q[:,j], A[:,i]) * Q[:,j]\n","            R[j,i] = np.dot(Q[:,j],A[:,i])\n","\n","        norm_q = np.linalg.norm(Q[:,i])\n","        Q[:,i] = Q[:,i] / norm_q\n","        R[i,i] = norm_q\n","\n","        ## linearly dependence\n","        if norm_q == 0:\n","            print(\"it has linearly dependent columns\")\n","            break\n","\n","    return Q, R"],"metadata":{"id":"iJKYaCaor8nu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# your code here\n","it_is_all_real = 0\n","\n","for i in range(1000):\n","    A = np.random.rand(100,10)\n","    A_gram = np.dot(A.T,A)\n","\n","    w, v = np.linalg.eig(A_gram)\n","\n","    w_r = np.real(w)\n","    w_i = w - w_r\n","\n","    is_it_all_real = np.linalg.norm(w_i)\n","\n","    if (is_it_all_real < 1e-12):\n","        it_is_all_real += 1\n","            \n","print(f\"{it_is_all_real} matrices are has all real eigenvalues\")\n","print(f\"The portion of all is {it_is_all_real / 10} %\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IcgC8RV_j07O","executionInfo":{"status":"ok","timestamp":1656712103631,"user_tz":-540,"elapsed":328,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"7b0423dc-668d-4710-c9a8-facb49c0e976"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1000 matrices are has all real eigenvalues\n","The portion of all is 100.0 %\n"]}]},{"cell_type":"code","source":["!pip install pycuda"],"metadata":{"id":"c0EUCxQIuV-q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656765880685,"user_tz":-540,"elapsed":111577,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"fc66c4d9-7b9e-4a4b-eeda-c915174ef713"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pycuda\n","  Downloading pycuda-2022.1.tar.gz (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 31.7 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting mako\n","  Downloading Mako-1.2.1-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 8.3 MB/s \n","\u001b[?25hCollecting pytools>=2011.2\n","  Downloading pytools-2022.1.12.tar.gz (70 kB)\n","\u001b[K     |████████████████████████████████| 70 kB 8.7 MB/s \n","\u001b[?25hRequirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.4.4)\n","Collecting platformdirs>=2.2.0\n","  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n","Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (4.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (2.0.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->mako->pycuda) (3.8.0)\n","Building wheels for collected packages: pycuda, pytools\n","  Building wheel for pycuda (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycuda: filename=pycuda-2022.1-cp37-cp37m-linux_x86_64.whl size=629484 sha256=d6eb77e308eaa95ccb92ee756a1d3be691df7c3b1581ed3d9cc2bbd1c8f4ce41\n","  Stored in directory: /root/.cache/pip/wheels/17/53/c9/caa05618e686df51f017d8a9923f38d915ce31df67ab6628e6\n","  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytools: filename=pytools-2022.1.12-py2.py3-none-any.whl size=65034 sha256=82fd0c5fe2ec8dc205babc8029c740b5f782572c692848d743e01f68628a0a00\n","  Stored in directory: /root/.cache/pip/wheels/37/5e/9e/76d7430e116b7cab0016fbabb26b896daae1946a3f7dea9915\n","Successfully built pycuda pytools\n","Installing collected packages: platformdirs, pytools, mako, pycuda\n","Successfully installed mako-1.2.1 platformdirs-2.5.2 pycuda-2022.1 pytools-2022.1.12\n"]}]},{"cell_type":"code","source":["import pycuda.autoinit\n","import pycuda.driver as drv\n","from pycuda import gpuarray\n","from pycuda.compiler import SourceModule\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from time import time\n","import math"],"metadata":{"id":"mzV-tH2fyUBs","executionInfo":{"status":"ok","timestamp":1656765881128,"user_tz":-540,"elapsed":452,"user":{"displayName":"이도훈","userId":"08244818386454187914"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["```python\n","import pycuda.autoinit\n","\n","calculate_ker = SourceModule(\n","    \"\"\"\n","    #define _X (threadIdx.x + blockIdx.x * blockDim.x * blockDim.y)\n","\n","    #define _WIDTH (blockDim.x)\n","\n","    #define _INDEX1(x,y) (x * _WIDTH + y)\n","\n","    // A: matrix, b: vector, out: vector\n","    __global__ void mat_vec_ker(float *out, float *A, float *b, float *theta)\n","    {\n","        int x = _X;\n","\n","        for (int j = 0; j < _WIDTH; j++)\n","        {\n","            // out need to be initialized\n","            out[x] += A[_INDEX1(x,j)] * theta[j];\n","        }\n","\n","        __syncthreads();\n","        out[x] -= b[x];\n","        __syncthreads();\n","    }\n","    \"\"\"\n",")\n","\n","gradient_ker = SourceModule(\n","    \"\"\"\n","    #define _X (threadIdx.x)\n","    #define _B (blockIdx.x)\n","    #define _G (gridDim.x)\n","\n","    #define _WIDTH (blockDim.x)\n","\n","    // grad_jerk: [BD,n], n = gridDim.x\n","    __global__ void gradient_ker(float *grad_jerk, float *out, float *A, int width)\n","    {\n","        int x = _X;\n","        int index_g = x * _G + _B;\n","        int index_a;\n","        int index_o;\n","\n","        for (int k = 0; k < _WIDTH; k++)\n","        {\n","            index_a = x + k * _WIDTH + _B * _WIDTH * _WIDTH;\n","            index_o = k + _B * _WIDTH;\n","\n","            grad_jerk[index_g] += A[index_a] * out[index_o];\n","        }\n","\n","        __syncthreads();\n","    }\n","\n","    __global__ void finish_ker(float *grad, float *grad_jerk)\n","    {\n","        int x = _X;\n","\n","        for (int k = 0; k < _G; k++)\n","        {\n","            int index = x * _G + k;\n","            grad[x] += grad_jerk[index];\n","        }\n","        __syncthreads();\n","    }\n","    \"\"\"\n",")\n","\n","update_ker = SourceModule(\n","    \"\"\"\n","    #define _X (threadIdx.x + blockIdx.x * blockDim.x * blockDim.y)\n","\n","    __global__ void update_ker(float *theta_new, float *theta, float lr, float *grad)\n","    {\n","        int x = _X;\n","\n","        theta_new[x] = theta[x] - grad[x] * lr;\n","\n","        __syncthreads();\n","    }\n","    \"\"\"\n",")\n","\n","multiply = calculate_ker.get_function(\"mat_vec_ker\")\n","gradient = gradient_ker.get_function(\"gradient_ker\")\n","finish = gradient_ker.get_function(\"finish_ker\")\n","update = update_ker.get_function(\"update_ker\")\n","```"],"metadata":{"id":"anQcHM38xZM4"}},{"cell_type":"code","source":["def optimal_block_size(n):\n","    \n","    thread_per_block = int(math.sqrt(n / 2))\n","\n","    block_per_grid = int(n / thread_per_block) + 1\n","\n","\n","    return thread_per_block, block_per_grid\n","\n","\n","\n","## block=(thread_per_block,1,1), grid=(block_per_grid,1,1)\n","first_ker = SourceModule(\n","    \"\"\"\n","    #define x (threadIdx.x + blockIdx.x * blockDim.x)\n","\n","    __global__ void first_ker(float* out, float* A, float* theta, int length, int width) {\n","        \n","        if (x < length) {\n","            for (int j = 0; j < width;, j++) {\n","                int index = x * width + j;\n","\n","                out[x] += A[index] * theta[j];\n","            }\n","        }\n","    }\n","    \"\"\"\n",")\n","\n","\n","\n","## block=(thread_per_block,1,1), grid=(block_per_grid,1,1)\n","second_ker = SourceModule(\n","    \"\"\"\n","    #define x (threadIdx.x + blockIdx.x * blockDim.x)\n","\n","    __global__ void second_ker(float* out, float* b, int length) {\n","\n","        if (x < length) {\n","            out[x] = out[x] - b[x];\n","        }\n","    }\n","    \"\"\"\n",")\n","\n","\n","\n","## block=(block_per_grid,1,1), grid=(width,1,1)\n","third_ker = SourceModule(\n","    \"\"\"\n","    __global__ void third_ker(float* grad, float* A, float* out, int thread_per_block, int block_per_grid, int width) {\n","\n","        __shared__ float* grad_jerk[gridDim.x];\n","\n","        for (int i = 0; i < thread_per_block; i++) {\n","            int index1 = threadIdx.x * block_per_grid + i;\n","            int index2 = index1 * gridDim.x + blockIdx.x;\n","             \n","            grad_jerk[threadIdx.x] += A[index2] * out[index1];\n","        }\n","\n","        if (threadIdx.x == 0) {\n","            for (int i = 0; i < gridDim.x; i++) {\n","                grad[x] += grad_jerk[i];\n","            }\n","        }\n","    }\n","    \"\"\"\n",")\n","\n","\n","\n","## block=(width,1,1), grid=(1,1,1)\n","gradient_method_ker = SourceModule(\n","    \"\"\"\n","    #define x (threadIdx.x)\n","\n","    __global__ void gradient_method (float* n_theta, float* theta, float* grad, float learning_rate) {\n","        n_theta[x] = theta[x] - learning_rate * grad[x];\n","    }\n","    \"\"\"\n",")\n","\n","\n","\n","## block=(width,1,1), grid=(1,1,1)\n","momentum_method_ker = SourceModule(\n","    \"\"\"\n","    #define x (threadIdx.x)\n","\n","    __global__ void momentum_method (float* n_theta, float* theta, float* grad, float* n_momentum, float* momentum, float s, float beta) {\n","        n_theta[x] = theta[x] - s * momentum[x];\n","        n_momentum[x] = grad[x] + beta * momentum[x];\n","    }\n","    \"\"\"\n",")"],"metadata":{"id":"BAHSCXqI9w3b"},"execution_count":null,"outputs":[]}]}