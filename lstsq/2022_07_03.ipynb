{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022_07_03.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOfbD5ujWQvYMw7Wk8UrhMC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sTOWaSWH8rJV","executionInfo":{"status":"ok","timestamp":1656847477733,"user_tz":-540,"elapsed":3150,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"a9a673e0-3d84-43fc-9ebf-2b2a95d8db53"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pycuda in /usr/local/lib/python3.7/dist-packages (2022.1)\n","Requirement already satisfied: mako in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.2.1)\n","Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.4.4)\n","Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.7/dist-packages (from pycuda) (2022.1.12)\n","Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (4.1.1)\n","Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (2.5.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (4.11.4)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (2.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->mako->pycuda) (3.8.0)\n"]}],"source":["!pip install pycuda"]},{"cell_type":"code","source":["import pycuda.autoinit\n","import pycuda.driver as drv\n","from pycuda import gpuarray\n","from pycuda.compiler import SourceModule\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from time import time\n","import math"],"metadata":{"id":"l-A3zrQl8yWO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Shared:\n","    def __init__(self, A, b, learning_rate):\n","        ## input matrix's shape\n","        self.length = A.shape[0]\n","        self.width = A.shape[1]\n","        \n","        ## in CPU\n","        self.A = np.float32(A)\n","        self.b = np.float32(b)\n","        self.theta = np.float32(np.zeros(self.width))\n","        self.out = np.float32(np.zeros(self.length))\n","        self.grad = np.float32(np.zeros(self.width))\n","\n","        ## in GPU\n","        self.GPU_A = gpuarray.to_gpu(self.A.reshape(self.length*self.width))\n","        self.GPU_b = gpuarray.to_gpu(self.b)\n","        self.GPU_theta = gpuarray.to_gpu(self.theta)\n","        self.GPU_out = gpuarray.to_gpu(self.out)\n","        self.GPU_grad = gpuarray.to_gpu(self.grad)\n","\n","        ## for initialize out vector and grad vector\n","        self.init_out = gpuarray.empty_like(self.GPU_out)\n","        self.init_grad = gpuarray.empty_like(self.GPU_grad)\n","\n","        ## TPB: thread_per_block, BPG: block_per_grid        \n","        self.TPB, self.BPG = self.optimal_block_size(self.length)\n","\n","        ## learning_rate\n","        self.learning_rate = learning_rate\n","\n","    def optimal_block_size(self, n):\n","        \n","        thread_per_block = int(math.sqrt(n / 2))\n","\n","        block_per_grid = int(n / thread_per_block) + 1\n","\n","\n","        return thread_per_block, block_per_grid"],"metadata":{"id":"KJ-KufQgWx8R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GetGradient:\n","    def __init__(self, shared):\n","       self.shared = shared\n","       \n","       self.kernel_function()\n","\n","    def run(self):\n","        ## get out = np.dot(A, theta)\n","        self.first(self.shared.GPU_out,\n","                   self.shared.GPU_A,\n","                   self.shared.GPU_theta,\n","                   np.int32(self.shared.length),\n","                   np.int32(self.shared.width),\n","                   block=(self.shared.TPB,1,1),\n","                   grid=(self.shared.BPG,1,1))\n","\n","        ## get out -= b\n","        self.second(self.shared.GPU_out,\n","                    self.shared.GPU_b,\n","                    np.int32(self.shared.length),\n","                    block=(self.shared.TPB,1,1),\n","                    grid=(self.shared.BPG,1,1))\n","\n","        ## get grad = np.dot(A.T, out)\n","        self.third(self.shared.GPU_grad,\n","                   self.shared.GPU_A,\n","                   self.shared.GPU_out,\n","                   np.int32(self.shared.TPB),\n","                   np.int32(self.shared.BPG),\n","                   np.int32(self.shared.length),\n","                   np.int32(self.shared.width),\n","                   block=(self.shared.BPG,1,1),\n","                   grid=(self.shared.width,1,1))\n","                   \n","                   \n","\n","    def kernel_function(self):\n","        ## block=(thread_per_block,1,1), grid=(block_per_grid,1,1)\n","        first_ker_function = \\\n","        \"\"\"\n","        #define x (threadIdx.x + blockIdx.x * blockDim.x)\n","\n","        __global__ void first(float* out, float* A, float* theta, int length, int width) {\n","            \n","            if (x < length) {\n","                for (int j = 0; j < width; j++) {\n","                    int index1 = x * width + j;\n","\n","                    out[x] += A[index1] * theta[j];\n","                    }\n","            }\n","        }\n","        \"\"\"\n","        first_ker = SourceModule(first_ker_function)\n","\n","\n","\n","        ## block=(thread_per_block,1,1), grid=(block_per_grid,1,1)\n","        second_ker_function = \\\n","        \"\"\"\n","        #define x (threadIdx.x + blockIdx.x * blockDim.x)\n","\n","        __global__ void second(float* out, float* b, int length) {\n","\n","            if (x < length) {\n","                out[x] = out[x] - b[x];\n","            }\n","        }\n","        \"\"\"\n","        second_ker = SourceModule(second_ker_function)\n","\n","\n","\n","        ## block=(block_per_grid,1,1), grid=(width,1,1)\n","        third_ker_function = \\\n","        \"\"\"\n","        #define tx (threadIdx.x)\n","        #define bx (blockIdx.x)\n","\n","        __global__ void third(float* grad, float* A, float* out, int thread_per_block, int block_per_grid, int length, int width) {\n","\n","            __shared__ float grad_jerk[1000];\n","\n","            grad_jerk[tx] = 0;\n","\n","            __syncthreads();\n","            \n","            for (int i = 0; i < thread_per_block; i++) {\n","                int index1 = tx * thread_per_block + i;\n","                int index2 = index1 * width + bx;\n","                \n","                grad_jerk[tx] += A[index2] * out[index1];\n","            }\n","\n","            __syncthreads();\n","\n","            if (tx == 0) {\n","                for (int i = 0; i < block_per_grid; i++) {\n","                    grad[bx] += grad_jerk[i];\n","                }\n","            }\n","            else {\n","                grad_jerk[1000-tx] = 0;\n","            }\n","\n","            __syncthreads();\n","        }\n","        \"\"\"\n","        third_ker = SourceModule(third_ker_function)\n","\n","        self.first = first_ker.get_function(\"first\")\n","        self.second = second_ker.get_function(\"second\")\n","        self.third = third_ker.get_function(\"third\")"],"metadata":{"id":"k1IeL-Oe-FPf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GradientMethod:\n","    def __init__(self, shared):\n","        self.shared = shared\n","\n","        self.kernel_function()\n","\n","    def run(self):\n","\n","        self.gradient_method(self.shared.GPU_theta,\n","                             self.shared.GPU_grad,\n","                             np.float32(self.shared.learning_rate),\n","                             block=(self.shared.width,1,1),\n","                             grid=(1,1,1))\n","\n","    def kernel_function(self):\n","\n","        ## block=(width,1,1), grid=(1,1,1)\n","        gradient_method_ker = SourceModule(\n","            \"\"\"\n","            #define x (threadIdx.x)\n","\n","            __global__ void gradient_method (float* theta, float* grad, float learning_rate) {\n","                theta[x] -= learning_rate * grad[x];\n","\n","                __syncthreads();\n","            }\n","            \"\"\"\n","        )\n","\n","        self.gradient_method = gradient_method_ker.get_function(\"gradient_method\")"],"metadata":{"id":"_TcO0IpEWmiu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MomentumMethod:\n","    def __init__(self, shared):\n","        self.shared = shared\n","\n","        self.kernel_function()\n","\n","    def run(self):\n","        \n","        self.momentum_method(self.shared.GPU_theta,\n","                             self.shared.GPU_grad,\n","                             self.shared.GPU_momentum,\n","                             np.float32(self.shared.s),\n","                             np.float32(self.shared.beta),\n","                             block=(self.shared.width,1,1),\n","                             grid=(1,1,1))\n","\n","    def kernel_function(self):\n","\n","        ## block=(width,1,1), grid=(1,1,1)\n","        momentum_method_ker = SourceModule(\n","            \"\"\"\n","            #define x (threadIdx.x)\n","\n","            __global__ void momentum_method (float* n_theta, float* theta, float* grad, float* n_momentum, float* momentum, float s, float beta) {\n","                n_theta[x] = theta[x] - s * momentum[x];\n","\n","                n_momentum[x] = grad[x] + beta * momentum[x];\n","\n","                __syncthreads();\n","            }\n","            \"\"\"\n","        )\n","        \n","        self.momentum_method = momentum_method_ker.get_function(\"momentum_method\")"],"metadata":{"id":"btD_DHPcWr5G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A = np.random.randn(1000,20)\n","b = np.random.randn(1000)\n","learning_rate = 2e-4\n","\n","shared = Shared(A, b, learning_rate)\n","\n","get_gradient = GetGradient(shared)\n","\n","get_gradient.run()"],"metadata":{"id":"Ll8QOnnUYejc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grad_cpu = np.dot(A.T, (np.dot(A, np.zeros(A.shape[1])) - b))\n","np.linalg.norm(shared.GPU_grad.get() - grad_cpu)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sye93_y9oIm4","executionInfo":{"status":"ok","timestamp":1656848203876,"user_tz":-540,"elapsed":473,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"e8a44a99-5e2c-48ca-b058-a0aaeb7eed4e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.2414368471636293e-05"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["gradient_method = GradientMethod(shared)\n","\n","gradient_method.run()"],"metadata":{"id":"rbKKILnru_vk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.linalg.norm(shared.GPU_theta.get() - (np.zeros(A.shape[1]) - learning_rate * grad_cpu))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FLdGdkdq1esk","executionInfo":{"status":"ok","timestamp":1656848266178,"user_tz":-540,"elapsed":7,"user":{"displayName":"이도훈","userId":"08244818386454187914"}},"outputId":"4cbb4b8d-9ab0-4bf9-b523-2523cd79db34"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.5706114221004795e-09"]},"metadata":{},"execution_count":20}]}]}